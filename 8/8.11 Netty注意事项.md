## 8.11 Netty注意事项

### 8.11.1 autoRead
在Netty有一个选项autoRead（默认为true），如果是true则当fireChannelActive事件触发时，会帮我们注册具体事件：对于ServerSocketChannel则为selectionKey.OP_ACCEPT；对应于SocketChannel则为selectionKey.OP_READ。当注册了读事件后，如果网络可读，就会从channel读取数据，然后触发fireChannelRead事件，在pipeline流动起来。那如果autoRead关掉后，则Netty不会注册具体事件，对于SocketChannel则为selectionKey.OP_READ事件，这样即使是对端发送数据过来了也不会触发读事件，从而也不会从channel读取到数据。那么这样一个功能主要用途是什么呢？

它的作用是速率控制：比如使用Netty开发一个应用，这个应用从网络上发送过来的数据量非常大，大到有时我们处理不过来了。而我们使用Netty开发应用往往是这样的安排方式：Netty的Worker线程处理IO事件，比如read和write，然后将读取后的数据交给pipeline处理，比如经过编解码等最后到业务层。到业务层的时候如果业务层有阻塞操作，比如数据库IO等，可能还要将收到的数据交给另外一个线程池处理。因为我们不能阻塞Worker线程，一旦阻塞就会影响网络处理效率，这些Worker是所有网络处理共享的，如果这里阻塞了，可能影响很多channel的网络处理。

但是，如果把接到的数据交给另外一个线程池处理就又涉及另外一个问题：速率匹配。比如现在业务量实在太忙了，接收到很多数据交给线程池，就会就出现两种情况：
1. 由于开发的时候没有认真考虑，这个线程池使用了某些无界资源，最后导致资源无节制使用，整个系统crash掉。
```Java
//这种方式线程数是无界的，那么有可能创建大量的线程对系统稳定性造成影响
Executor executor = Executors.newCachedTheadPool();
executor.execute(requestWorker);
//这种queue是无界的，有可能会消耗太多内存，对系统稳定性造成影响
Executor executor = Executors.newFixedThreadPool(8);
executor.execute(requestWorker);
```
2. 第二种情况就是限制了资源使用，所以只好把最老的或最新的数据丢弃。
```Java
//线程池满后，将最老的数据丢弃
Executor executor = new ThreadPoolExecutor(8, 8, 1, TimeUnit.MINUTES, new ArrayBlockingQueue<Runnable>(1000), namedFactory, new ThreadPoolExecutor.DiscardOldestPolicy());
```
其实上面两种情况，不管哪一种都不是太合理，在Netty里我们就有了更好的解决办法了。如果我们的线程池暂时处理不过来，那么我们可以将autoRead关闭，这样Netty就不再从channel上读取数据了，那么这样造成的影响是什么呢？这样socket在内核那一层的read buffer就会满了。因为TCP默认就是带flow control的，read buffer变小之后，向对端发送ACK的时候，就会降低窗口大小，直至变成0，这样对端就会自动的降低发送数据的速率了。等到我们又可以处理数据了，我们就可以将autoRead又打开这样数据又源源不断的到来。

这样整个系统就通过TCP的这个负反馈机制，和谐的运行着。那么autoRead涉及的网络知识就是，发送端会根据对端ACK时候所携带的advertises window来调整自己发送的数据量。而ACK里的这个window的大小又跟接收端的read buffer有关系。而不注册读事件后，read buffer里的数据没有被消费掉，就会达到控制发送端速度的目的。

不过设计关闭和打开autoRead的策略也要注意，不要设计成我们不能处理任何数据了就立即关闭autoRead，而我们开始能处理了就立即打开autoRead。这样做有可能就会导致我们的autoRead频繁打开和关闭，autoRead的每次调整都会涉及系统调用，对性能是有影响的。正确做法是应该留一个缓冲地带，也就是如果现在排队的数据达到我们预设置的一个高水位线的时候我们关闭autoRead，而低于一个低水位线的时候才打开autoRead。类似下面这样一个代码，在将任务提交到线程池之前，判断一下现在的排队量：
```Java
int highReadWaterMarker = 900;
int lowReadWaterMarker = 600;

ThreadPoolExecutor executor = new ThreadPoolExecutor(8, 8, 1, TimeUnit.MINUTES, new ArrayBlockingQueue<Runnable>(1000), namedFactory, new ThreadPoolExecutor.DiscardOldestPolicy());

int queued = executor.getQueue().size();
if(queued > highReadWaterMarker){
    channel.config().setAutoRead(false);
}
if(queued < lowReadWaterMarker){
    channel.config().setAutoRead(true);
}
```
但是使用autoRead也要注意一件事情。autoRead如果关闭后，对端发送FIN的时候，接收端应用层也是感知不到的。这样带来一个后果就是对端发送了FIN，然后内核将这个socket的状态变成CLOSE_WAIT。但是因为应用层感知不到，所以应用层一直没有调用close。这样的socket就会长期处于CLOSE_WAIT状态。特别是一些使用连接池的应用，如果将连接归还给连接池后，一定要记着autoRead一定是打开的。不然就会有大量的连接处于CLOSE_WAIT状态。

需要注意的是：其实所有异步的场合都存在速率匹配的问题，而同步往往不存在这样的问题，因为同步本身就是带负反馈的。

### 8.11.2 isWritable
上面我们讲的autoRead一般是接收端的事情，而发送端也有速率控制的问题。前面我们说过，在调用channel.write的时候，数据并不是直接发送到网络，而是先写到ChannelOutboundBuffer，当调用channel.flush的时候才将数据发送到网络。因为这中间有一个buffer，就存在速率匹配了，而且这个buffer还是无界的，也就是如果不控制channel.write的速度，会有大量的数据在这个buffer里堆积，而且如果碰到socket又“发不出”数据（比如对端通告窗口为0）的时候，其结果可能会耗尽资源。而且这里让这个事情更严重的是ChannelOutboundBuffer一般使用DirectByteBuffer，这些内存是放在GC Heap之外，即如果我们仅仅监控GC的话是监控不出来这个隐患。

Netty已经为我们考虑了这点。channel有一个isWritable属性，可以来控制ChannelOutboundBuffer，不让其无限制膨胀。

### 8.11.3 应用层缓冲区
NIO网络编程中应用层buffer是必须的，因为NIO的核心思想是避免阻塞在read/write等IO操作上，这样一个线程才可以让服务于多个socket连接。IO线程只能阻塞在IO-multiplexing函数上，如select/epoll等。这样一来，应用层的缓冲是必须的，每个socket都要有stateful的input buffer和output buffer。

#### output buffer
考虑一个常见场景：程序想通过TCP连接发送100k字节的数据，但是在write()调用中，操作系统只接受了80k字节（受本端内核发送buffer和对端接收buffer的限制）。而该发送操作又需要立刻返回，不能阻赛。此时网络库应该接管这剩余的20k字节数据，把它保存在该 TCP socket的output buffer里，然后注册SelectionKey.OP_WRITE事件，一旦socket变得可写就立刻发送数据。如果发送完着20k字节数据，则应该注销SelectionKey.OP_WRITE事件，以免造成busy loop；否则继续关注OP_WRITE事件，直至发送完全。

综上，要让应用程序在write操作上不阻塞，网络库必须要给每个tcp connection配置output buffer。

#### input buffer
TCP是一个无边界的字节流协议，接收方必须要处理“收到的数据尚不构成一条完整的消息”和“一次收到两条消息的数据”等情况。而网络库在处理“socket可读”事件的时候，必须一次性把socket里的数据读完（从操作系统buffer copy到应用层 buffer），否则会反复触发OP_READ事件，造成 busy-loop。

那么网络库就必须应对“数据不完整”的情况，收到的数据先放到input buffer里，等构成一条完整的消息再通知程序的业务逻辑。这通常是ByteToMessageCodec的职责。所以，网络库必须要给每个tcp connection配置input buffer。
